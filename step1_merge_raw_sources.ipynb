{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Physical Activity and Sedentary Data\n",
    "Atleast 3 valid days with wear time ≥ 600 mins are required for each subject to be included in the summary. The summary includes the mean MVPA and sedentary time for each subject across their valid days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   mean_mvpa  mean_sedentary  valid_days\n",
      "0   2  484.027778      812.777778           6\n",
      "1   3  473.333333      824.261905           7\n",
      "2   4  443.380952      867.761905           7\n",
      "3   7  397.125000      502.375000           4\n",
      "4   9  478.571429      785.309524           7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel(\"raw_databases/actigraph_raw.xlsx\", sheet_name=\"in\")\n",
    "\n",
    "# Step 1: Remove rows where Calendar Days == 0\n",
    "df = df[df[\"Calendar Days\"] != 0]\n",
    "\n",
    "# Step 2: Keep only rows with valid wear time (Time ≥ 600 mins)\n",
    "df_valid = df[df[\"Time\"] >= 600]\n",
    "\n",
    "# Step 3: Identify subjects with at least 3 valid days\n",
    "valid_subjects = (\n",
    "    df_valid.groupby(\"id\")[\"Date\"]\n",
    "    .count()\n",
    "    .reset_index(name=\"valid_days\")\n",
    "    .query(\"valid_days >= 3\")[\"id\"]\n",
    ")\n",
    "\n",
    "# Step 4: Filter again to include only these valid subjects\n",
    "df_valid_filtered = df_valid[df_valid[\"id\"].isin(valid_subjects)]\n",
    "\n",
    "# Step 5: Compute average MVPA and sedentary time across valid days\n",
    "df_summary = df_valid_filtered.groupby(\"id\").agg(\n",
    "    mean_mvpa=(\"Total MVPA\", \"mean\"),\n",
    "    mean_sedentary=(\"Sedentary\", \"mean\"),\n",
    "    valid_days=(\"Date\", \"count\")\n",
    ").reset_index()\n",
    "\n",
    "# Optional: Save or view result\n",
    "df_summary.to_csv(\"mvpa_sedentary_summary.csv\", index=False)\n",
    "print(df_summary.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Sleep Data\n",
    "Similar to physical activity, atleast 3 days of valid data. Averaged sleep fragmentation across valid days. Sleep fragmentation done as in : https://doi.org/10.1002/oby.23754"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"raw_databases/sleep.csv\")\n",
    "\n",
    "# Group by 'Subject Name' and calculate the average Sleep Fragmentation Index\n",
    "sleep_summary = df.groupby('Subject Name')['Sleep Fragmentation Index'].mean().reset_index()\n",
    "\n",
    "# Rename the columns as required\n",
    "sleep_summary.columns = ['id', 'avg_sleep_frag_index']\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "sleep_summary.to_csv(\"sleep_summary.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Meal Microstructure\n",
    "Microstructure coded from protocol: https://doi.org/10.5281/zenodo.8140895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: microstructure_summary.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load your actual dataset\n",
    "df = pd.read_csv(\"raw_databases/micro_beh_summary.csv\")\n",
    "\n",
    "# Ensure numeric columns\n",
    "df[\"ps\"] = pd.to_numeric(df[\"ps\"], errors=\"coerce\")\n",
    "df[\"id\"] = pd.to_numeric(df[\"id\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"id\", \"ps\"])\n",
    "\n",
    "# Use fallback logic where needed\n",
    "df[\"total_active_eating\"] = pd.to_numeric(df[\"total_active_eating_c1\"], errors=\"coerce\").combine_first(\n",
    "    pd.to_numeric(df[\"total_active_eating_c2\"], errors=\"coerce\"))\n",
    "df[\"meal_duration\"] = pd.to_numeric(df[\"meal_duration_c1\"], errors=\"coerce\").combine_first(\n",
    "    pd.to_numeric(df[\"meal_duration_c2\"], errors=\"coerce\"))\n",
    "df[\"bite_rate\"] = pd.to_numeric(df[\"bite_rate_c1\"], errors=\"coerce\").combine_first(\n",
    "    pd.to_numeric(df[\"bite_rate_active_c1\"], errors=\"coerce\")).combine_first(\n",
    "    pd.to_numeric(df[\"bite_rate_active_c2\"], errors=\"coerce\"))\n",
    "\n",
    "# Compute % active eating time\n",
    "df[\"percent_active_eating\"] = df[\"total_active_eating\"] / df[\"meal_duration\"]\n",
    "\n",
    "# Prepare result list\n",
    "results = []\n",
    "\n",
    "for child_id, group in df.groupby(\"id\"):\n",
    "    entry = {\"id\": child_id}\n",
    "    \n",
    "    # Mean bite rate: keep if at least one value\n",
    "    bite_rate_vals = group[\"bite_rate\"].dropna()\n",
    "    if len(bite_rate_vals) >= 1:\n",
    "        entry[\"bite_rate_mean\"] = bite_rate_vals.mean()\n",
    "    \n",
    "    # Slope & intercept of % active eating time: require ≥2 PS values\n",
    "    X = group[\"ps\"].values.reshape(-1, 1)\n",
    "    y = group[\"percent_active_eating\"].values\n",
    "    valid = np.isfinite(X.ravel()) & np.isfinite(y)\n",
    "\n",
    "    if valid.sum() >= 2:\n",
    "        model = LinearRegression().fit(X[valid], y[valid])\n",
    "        entry[\"percent_active_eating_slope\"] = model.coef_[0]\n",
    "        entry[\"percent_active_eating_intercept\"] = model.intercept_\n",
    "    else:\n",
    "        # skip slope/intercept if not enough points\n",
    "        continue\n",
    "\n",
    "    results.append(entry)\n",
    "\n",
    "# Output summary\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df.to_excel(\"microstructure_summary.xlsx\", index=False)\n",
    "print(\"Saved as: microstructure_summary.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Bite Size\n",
    "Bite size calculated as intake(grams) at each portion size / number of bites at respective portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  ps    gram  nbites  bite_size_gram\n",
      "0   1   1  615.80     117        5.263248\n",
      "1   2   1  311.32      51        6.104314\n",
      "2   3   1  382.28      68        5.621765\n",
      "3   4   1  689.83      92        7.498152\n",
      "4   5   1  657.45      75        8.766000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "gram_data = pd.read_csv(\"raw_databases/intake_data.csv\")\n",
    "bites_data = pd.read_csv(\"raw_databases/micro_beh_summary.csv\")\n",
    "\n",
    "# Step 1: Melt gram data from wide to long format\n",
    "gram_long = pd.melt(\n",
    "    gram_data,\n",
    "    id_vars=[\"id\"],\n",
    "    value_vars=[\"ps1_total_g\", \"ps2_total_g\", \"ps3_total_g\", \"ps4_total_g\"],\n",
    "    var_name=\"ps_label\",\n",
    "    value_name=\"gram\"\n",
    ")\n",
    "\n",
    "# Step 2: Extract PS number\n",
    "gram_long[\"ps\"] = gram_long[\"ps_label\"].str.extract(r\"ps(\\d+)_total_g\").astype(int)\n",
    "gram_long.drop(columns=\"ps_label\", inplace=True)\n",
    "\n",
    "# Step 3: Use preferred nbites column\n",
    "bites_data[\"nbites\"] = bites_data[\"nbites_c1\"].combine_first(bites_data[\"nbites_c2\"])\n",
    "\n",
    "# Step 4: Clean and convert 'ps' column in bites_data\n",
    "bites_data[\"ps\"] = pd.to_numeric(bites_data[\"ps\"], errors='coerce')\n",
    "bites_data = bites_data.dropna(subset=[\"ps\"])\n",
    "bites_data[\"ps\"] = bites_data[\"ps\"].astype(int)\n",
    "\n",
    "# Step 5: Merge gram and bites data\n",
    "merged = gram_long.merge(\n",
    "    bites_data[[\"id\", \"ps\", \"nbites\"]],\n",
    "    on=[\"id\", \"ps\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 6: Clean data and calculate bite size\n",
    "merged[\"nbites\"] = pd.to_numeric(merged[\"nbites\"], errors=\"coerce\")\n",
    "merged[\"gram\"] = pd.to_numeric(merged[\"gram\"], errors=\"coerce\")\n",
    "merged = merged[merged[\"nbites\"].notna() & (merged[\"nbites\"] != 0)]\n",
    "merged = merged[merged[\"gram\"].notna()]\n",
    "merged[\"bite_size_gram\"] = merged[\"gram\"] / merged[\"nbites\"]\n",
    "\n",
    "# Step 7: Keep relevant columns\n",
    "result = merged[[\"id\", \"ps\", \"gram\", \"nbites\", \"bite_size_gram\"]]\n",
    "\n",
    "# Output\n",
    "print(result.head())\n",
    "result.to_csv(\"matched_gram_bites_bitesize.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finding slopes and intercept for bite size data\n",
    "Slope and intercepts calculated per subject across portion sizes : served portion sizes vs bite size at each portion size. Bite size increases with portions, and therefore slope and intercept captures that change with portion sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  bite_size_slope  bite_size_int\n",
      "0   1        -0.207803       5.471051\n",
      "1   2        -0.642434       6.605830\n",
      "2   3        -0.401121       6.215653\n",
      "3   4        -0.353493       6.920628\n",
      "4   5        -0.346405       8.848133\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Load the previously saved bite size data (in grams)\n",
    "data = pd.read_csv(\"matched_gram_bites_bitesize.csv\")\n",
    "\n",
    "# Ensure relevant columns are numeric\n",
    "data[\"ps\"] = pd.to_numeric(data[\"ps\"], errors=\"coerce\")\n",
    "data[\"bite_size_gram\"] = pd.to_numeric(data[\"bite_size_gram\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing values in ps or bite_size_gram\n",
    "data = data.dropna(subset=[\"ps\", \"bite_size_gram\"])\n",
    "\n",
    "# Function to compute bite size slope and intercept per ID\n",
    "def compute_slope_intercept(group):\n",
    "    if group[\"ps\"].nunique() >= 2:\n",
    "        slope, intercept, *_ = linregress(group[\"ps\"], group[\"bite_size_gram\"])\n",
    "        return pd.Series({\"bite_size_slope\": slope, \"bite_size_int\": intercept})\n",
    "    else:\n",
    "        return pd.Series({\"bite_size_slope\": None, \"bite_size_int\": None})\n",
    "\n",
    "# Group by ID and apply regression\n",
    "regression_results = data.groupby(\"id\").apply(compute_slope_intercept).reset_index()\n",
    "\n",
    "# Output\n",
    "print(regression_results.head())\n",
    "regression_results.to_csv(\"bite_size_regression_per_id.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Switching Data Processing\n",
    "Switching calculated as per: https://doi.org/10.1016/j.physbeh.2023.114312\n",
    "Averaged across all portion sizes since it is a stable predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  fswitch_nok_mean\n",
      "0   1              9.50\n",
      "1   2             20.00\n",
      "2   3             11.25\n",
      "3   4             16.25\n",
      "4   5             26.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"raw_databases/Switching database.csv\")\n",
    "\n",
    "# List of PS columns to average\n",
    "ps_columns = [\n",
    "    \"ps1_fswitch_nok\",\n",
    "    \"ps2_fswitch_nok\",\n",
    "    \"ps3_fswitch_nok\",\n",
    "    \"ps4_fswitch_nok\"\n",
    "]\n",
    "\n",
    "# Ensure numeric and handle any non-numeric entries gracefully\n",
    "for col in ps_columns:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Calculate mean fswitch_nok per ID\n",
    "data[\"fswitch_nok_mean\"] = data[ps_columns].mean(axis=1)\n",
    "\n",
    "# Keep only relevant columns\n",
    "result = data[[\"id\", \"fswitch_nok_mean\"]]\n",
    "\n",
    "# Output\n",
    "print(result.head())\n",
    "result.to_csv(\"fswitch_nok_mean_per_id.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  F. Portion Size Response\n",
    "Slope and intercepts calculated per subject across portion sizes : served portion sizes vs intake at each portion size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  ps1_served_g  ps2_served_g  ps3_served_g  ps4_served_g  ps1_consumed_g  \\\n",
      "0   1        809.31       1047.75       1279.18       1522.45          441.42   \n",
      "1   2        802.58       1046.71       1271.43       1520.89          245.14   \n",
      "2   3        799.44       1043.93       1140.36       1481.64          291.89   \n",
      "3   4        799.87       1043.29       1265.84       1509.93          425.34   \n",
      "4   5        790.91       1038.25       1284.20       1512.65          591.18   \n",
      "\n",
      "   ps2_consumed_g  ps3_consumed_g  ps4_consumed_g  \n",
      "0          412.27          413.48          479.69  \n",
      "1          384.69          221.97          360.29  \n",
      "2          476.48          404.43          387.07  \n",
      "3          626.82          734.18          692.78  \n",
      "4          663.34          666.57          869.58  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "data = pd.read_excel(\"raw_databases/PS.xlsx\")\n",
    "\n",
    "# Define columns for each PS\n",
    "served_cols = {\n",
    "    \"ps1_served_g\": [\n",
    "        \"ps1_noplate_chkn_nug_g\",\n",
    "        \"ps1_noplate_mac_cheese_g\",\n",
    "        \"ps1_noplate_grapes_g\",\n",
    "        \"ps1_noplate_broccoli_g\",\n",
    "        \"ps1_noplate_ketchup_g\"\n",
    "    ],\n",
    "    \"ps2_served_g\": [\n",
    "        \"ps2_noplate_chkn_nug_g\",\n",
    "        \"ps2_noplate_mac_cheese_g\",\n",
    "        \"ps2_noplate_grapes_g\",\n",
    "        \"ps2_noplate_broccoli_g\",\n",
    "        \"ps2_noplate_ketchup_g\"\n",
    "    ],\n",
    "    \"ps3_served_g\": [\n",
    "        \"ps3_noplate_chkn_nug_g\",\n",
    "        \"ps3_noplate_mac_cheese_g\",\n",
    "        \"ps3_noplate_grapes_g\",\n",
    "        \"ps3_noplate_broccoli_g\",\n",
    "        \"ps3_noplate_ketchup_g\"\n",
    "    ],\n",
    "    \"ps4_served_g\": [\n",
    "        \"ps4_noplate_chkn_nug_g\",\n",
    "        \"ps4_noplate_mac_cheese_g\",\n",
    "        \"ps4_noplate_grapes_g\",\n",
    "        \"ps4_noplate_broccoli_g\",\n",
    "        \"ps4_noplate_ketchup_g\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "consumed_cols = {\n",
    "    \"ps1_consumed_g\": [\n",
    "        \"ps1_consumed_chkn_nug_g\",\n",
    "        \"ps1_consumed_mac_cheese_g\",\n",
    "        \"ps1_consumed_grapes_g\",\n",
    "        \"ps1_consumed_broccoli_g\",\n",
    "        \"ps1_consumed_ketchup_g\"\n",
    "    ],\n",
    "    \"ps2_consumed_g\": [\n",
    "        \"ps2_consumed_chkn_nug_g\",\n",
    "        \"ps2_consumed_mac_cheese_g\",\n",
    "        \"ps2_consumed_grapes_g\",\n",
    "        \"ps2_consumed_broccoli_g\",\n",
    "        \"ps2_consumed_ketchup_g\"\n",
    "    ],\n",
    "    \"ps3_consumed_g\": [\n",
    "        \"ps3_consumed_chkn_nug_g\",\n",
    "        \"ps3_consumed_mac_cheese_g\",\n",
    "        \"ps3_consumed_grapes_g\",\n",
    "        \"ps3_consumed_broccoli_g\",\n",
    "        \"ps3_consumed_ketchup_g\"\n",
    "    ],\n",
    "    \"ps4_consumed_g\": [\n",
    "        \"ps4_consumed_chkn_nug_g\",\n",
    "        \"ps4_consumed_mac_cheese_g\",\n",
    "        \"ps4_consumed_grapes_g\",\n",
    "        \"ps4_consumed_broccoli_g\",\n",
    "        \"ps4_consumed_ketchup_g\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Compute served totals with numeric conversion\n",
    "for new_col, cols in served_cols.items():\n",
    "    data[new_col] = data[cols].apply(pd.to_numeric, errors=\"coerce\").sum(axis=1)\n",
    "\n",
    "# Compute consumed totals with numeric conversion\n",
    "for new_col, cols in consumed_cols.items():\n",
    "    data[new_col] = data[cols].apply(pd.to_numeric, errors=\"coerce\").sum(axis=1)\n",
    "\n",
    "# Select and reorder final columns\n",
    "final_cols = [\"id\"] + list(served_cols.keys()) + list(consumed_cols.keys())\n",
    "final_data = data[final_cols]\n",
    "\n",
    "# Save to Excel\n",
    "final_data.to_excel(\"portion_weights_summary.xlsx\", index=False)\n",
    "\n",
    "# Show preview\n",
    "print(final_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  ps_response_slope  ps_response_int\n",
      "0   1           0.049458       379.112909\n",
      "1   2           0.081456       208.500184\n",
      "2   3           0.100569       277.698276\n",
      "3   4           0.385958       174.102320\n",
      "4   5           0.344736       298.979595\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Load previously saved portion weight summary\n",
    "data = pd.read_excel(\"portion_weights_summary.xlsx\")\n",
    "\n",
    "# Columns to extract and match\n",
    "served_cols = [\"ps1_served_g\", \"ps2_served_g\", \"ps3_served_g\", \"ps4_served_g\"]\n",
    "consumed_cols = [\"ps1_consumed_g\", \"ps2_consumed_g\", \"ps3_consumed_g\", \"ps4_consumed_g\"]\n",
    "\n",
    "# Ensure all relevant columns are numeric\n",
    "for col in served_cols + consumed_cols:\n",
    "    data[col] = pd.to_numeric(data[col], errors=\"coerce\")\n",
    "\n",
    "# Function to compute slope and intercept per child\n",
    "def compute_ps_response(row):\n",
    "    x = row[served_cols].values\n",
    "    y = row[consumed_cols].values\n",
    "\n",
    "    # Filter out missing pairs\n",
    "    valid = ~(pd.isna(x) | pd.isna(y))\n",
    "    x_valid = x[valid]\n",
    "    y_valid = y[valid]\n",
    "\n",
    "    if len(x_valid) >= 2:\n",
    "        slope, intercept, *_ = linregress(x_valid, y_valid)\n",
    "        return pd.Series({\"ps_response_slope\": slope, \"ps_response_int\": intercept})\n",
    "    else:\n",
    "        return pd.Series({\"ps_response_slope\": None, \"ps_response_int\": None})\n",
    "\n",
    "# Apply row-wise\n",
    "response_results = data[[\"id\"] + served_cols + consumed_cols].copy()\n",
    "response_results[[\"ps_response_slope\", \"ps_response_int\"]] = response_results.apply(compute_ps_response, axis=1)\n",
    "\n",
    "# Keep only ID and regression outputs\n",
    "final_results = response_results[[\"id\", \"ps_response_slope\", \"ps_response_int\"]].dropna()\n",
    "\n",
    "# Save to Excel\n",
    "final_results.to_excel(\"ps_response_slopes.xlsx\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(final_results.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Database\n",
    "Combining all predictors and data across all the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Merged dataset saved as 'merged_dataset.csv' with shape: (76, 53)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# ---------- Load individual datasets ----------\n",
    "\n",
    "# Helper function to load and cast 'id' to str\n",
    "def load_and_cast(filepath, usecols, filetype=\"csv\"):\n",
    "    if filetype == \"csv\":\n",
    "        df = pd.read_csv(filepath, usecols=usecols)\n",
    "    else:\n",
    "        df = pd.read_excel(filepath, usecols=usecols)\n",
    "    df[\"id\"] = df[\"id\"].astype(str)\n",
    "    return df\n",
    "\n",
    "# 1. MVPA and sedentary data\n",
    "mvpa = load_and_cast(\"mvpa_sedentary_summary.csv\", [\"id\", \"mean_mvpa\", \"mean_sedentary\"])\n",
    "\n",
    "# 2. Sleep fragmentation\n",
    "sleep = load_and_cast(\"sleep_summary.csv\", [\"id\", \"avg_sleep_frag_index\"])\n",
    "\n",
    "# 3. Microstructure summary\n",
    "microstructure = load_and_cast(\"microstructure_summary.xlsx\", [\"id\", \"percent_active_eating_slope\", \"percent_active_eating_intercept\"], filetype=\"excel\")\n",
    "\n",
    "# 4. Bite size regression\n",
    "bite_size = load_and_cast(\"bite_size_regression_per_id.csv\", [\"id\", \"bite_size_slope\", \"bite_size_int\"])\n",
    "\n",
    "# 5. FSwitch mean\n",
    "fswitch = load_and_cast(\"fswitch_nok_mean_per_id.csv\", [\"id\", \"fswitch_nok_mean\"])\n",
    "\n",
    "# 6. PS response slopes\n",
    "ps_response = load_and_cast(\"ps_response_slopes.xlsx\", [\"id\", \"ps_response_slope\", \"ps_response_int\"], filetype=\"excel\")\n",
    "\n",
    "# 7. Anthropometry\n",
    "anthro = load_and_cast(\"raw_databases/anthro_data.csv\", [\"id\", \"risk_status_mom\", \"sex\", \"age_yr\", \"parent_ed\", \"income\", \"bmi_percentile\", \"v7_bmi_percentile\"])\n",
    "\n",
    "# 8. Demographics\n",
    "demo = load_and_cast(\"raw_databases/demographics_data.csv\", [\"id\", \"pds_score\", \"pds_tanner_cat\"])\n",
    "\n",
    "# 9. Intake\n",
    "intake = load_and_cast(\"raw_databases/intake_data.csv\", [\"id\", \"v1_meal_total_kcal\", \"v1_eah_total_kcal\"])\n",
    "\n",
    "# 10. Behavioral questionnaires\n",
    "qs = pd.read_csv(\"raw_databases/qs_eatbeh_bodyimage.csv\")\n",
    "qs[\"id\"] = qs[\"id\"].astype(str)\n",
    "qs_cols = [\n",
    "    \"id\",\n",
    "    # CEBQ\n",
    "    \"cebq_fr\", \"cebq_eoe\", \"cebq_ef\", \"cebq_dd\", \"cebq_sr\", \"cebq_se\", \"cebq_eue\", \"cebq_ff\", \"cebq_approach\", \"cebq_avoid\",\n",
    "    # CFQ\n",
    "    \"cfq_resp\", \"cfq_pcw\", \"cfq_ppw\", \"cfq_cwc\", \"cfq_rest\", \"cfq_pressure\", \"cfq_mon\",\n",
    "    # FFBS\n",
    "    \"ffbs_control\", \"ffbs_presence\", \"ffbs_ch_choice\", \"ffbs_org\",\n",
    "    # PWLB\n",
    "    \"pwlb_healthy\", \"pwlb_unhealthy\",\n",
    "    # TFEQ\n",
    "    \"tfeq_cogcontrol\", \"tfeq_disinhibition\", \"tfeq_hunger\"\n",
    "]\n",
    "qs_selected = qs[qs_cols]\n",
    "\n",
    "# 11. Additional cognitive/psychosocial variables\n",
    "cog_psych = load_and_cast(\"raw_databases/qs_cog_psych_soc.csv\", [\n",
    "    \"id\",\n",
    "    \"bas_funseeking\", \"bas_drive\", \"bas_rewardresp\", \"bis\",\n",
    "    \"brief2_gec_p\"\n",
    "])\n",
    "\n",
    "# ---------- Merge all datasets ----------\n",
    "dfs = [\n",
    "    anthro, mvpa, sleep, microstructure, bite_size, fswitch, ps_response,\n",
    "    demo, intake, qs_selected, cog_psych\n",
    "]\n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on=\"id\", how=\"left\"), dfs)\n",
    "\n",
    "# ---------- Ensure v7_bmi_percentile is numeric ----------\n",
    "merged_df[\"v7_bmi_percentile\"] = pd.to_numeric(merged_df[\"v7_bmi_percentile\"], errors=\"coerce\")\n",
    "\n",
    "# ---------- Filter rows with valid v7_bmi_percentile ----------\n",
    "final_df = merged_df[merged_df[\"v7_bmi_percentile\"].notna()].reset_index(drop=True)\n",
    "\n",
    "# ---------- Save merged file ----------\n",
    "final_df.to_csv(\"merged_dataset.csv\", index=False)\n",
    "print(\"✅ Merged dataset saved as 'merged_dataset.csv' with shape:\", final_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Synthesizing Dataset\n",
    "Synthesizing pseduo-real-world dataset from current dataset - to increase sample size for tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sdv/single_table/base.py:145: FutureWarning:\n",
      "\n",
      "The 'SingleTableMetadata' is deprecated. Please use the new 'Metadata' class for synthesizers.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final dataset saved as 'ML_child_obesity_syn_data.csv'\n",
      "✅ Metadata saved as 'ctgan_metadata.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# --- Load dataset and clean blanks ---\n",
    "df = pd.read_csv(\"merged_dataset.csv\")\n",
    "df = df.replace(r\"^\\s*$\", np.nan, regex=True)\n",
    "\n",
    "# Separate ID and data\n",
    "df_id = df[\"id\"]\n",
    "df_data = df.drop(columns=[\"id\"])\n",
    "\n",
    "# --- Drop fully missing columns ---\n",
    "all_nan_cols = df_data.columns[df_data.isnull().all()]\n",
    "df_data = df_data.drop(columns=all_nan_cols)\n",
    "\n",
    "# --- Store missingness pattern ---\n",
    "missing_mask = df_data.isnull()\n",
    "\n",
    "# --- Impute missing values ---\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df_data), columns=df_data.columns)\n",
    "\n",
    "# --- Categorical columns ---\n",
    "categorical_columns = [\n",
    "    \"sex\", \"risk_status_mom\", \"income\", \"parent_ed\", \"pds_tanner_cat\"\n",
    "]\n",
    "\n",
    "# --- Coerce types ---\n",
    "for col in categorical_columns:\n",
    "    if col in df_imputed.columns:\n",
    "        df_imputed[col] = df_imputed[col].astype(int)\n",
    "\n",
    "for col in df_imputed.columns:\n",
    "    if col not in categorical_columns:\n",
    "        df_imputed[col] = pd.to_numeric(df_imputed[col], errors='coerce')\n",
    "\n",
    "# --- Metadata ---\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(df_imputed)\n",
    "\n",
    "manual_dtypes = {col: \"categorical\" for col in categorical_columns}\n",
    "for col in df_imputed.columns:\n",
    "    if col not in categorical_columns:\n",
    "        manual_dtypes[col] = \"numerical\"\n",
    "\n",
    "for col, dtype in manual_dtypes.items():\n",
    "    if col in metadata.columns:\n",
    "        metadata.update_column(column_name=col, sdtype=dtype)\n",
    "\n",
    "if os.path.exists(\"ctgan_metadata.json\"):\n",
    "    os.remove(\"ctgan_metadata.json\")\n",
    "metadata.save_to_json(\"ctgan_metadata.json\")\n",
    "\n",
    "# --- Fit model ---\n",
    "synthesizer = CTGANSynthesizer(metadata, epochs=300)\n",
    "synthesizer.fit(df_imputed)\n",
    "\n",
    "# --- Generate data ---\n",
    "n_original = df_data.shape[0]\n",
    "n_to_generate = 350 - n_original\n",
    "synthetic_new = synthesizer.sample(num_rows=n_to_generate)\n",
    "\n",
    "# --- Clip known bounds ---\n",
    "clip_bounds = {\n",
    "    \"v1_meal_total_kcal\": (200, 3000),\n",
    "    \"v1_eah_total_kcal\": (0, 2000),\n",
    "    \"age_yr\": (2, 12),\n",
    "    \"bmi_percentile\": (15, 95),\n",
    "    \"v7_bmi_percentile\": (15, 95),\n",
    "    \"sex\": (0, 1),\n",
    "    \"risk_status_mom\": (0, 1),\n",
    "    \"income\": (0, 5),\n",
    "    \"parent_ed\": (0, 5),\n",
    "    \"pds_tanner_cat\": (1, 2)\n",
    "}\n",
    "for col, (low, high) in clip_bounds.items():\n",
    "    if col in synthetic_new.columns:\n",
    "        synthetic_new[col] = pd.to_numeric(synthetic_new[col], errors='coerce')\n",
    "        synthetic_new[col] = synthetic_new[col].clip(lower=low, upper=high)\n",
    "        if col in categorical_columns:\n",
    "            synthetic_new[col] = synthetic_new[col].round().astype(int)\n",
    "\n",
    "# --- Enforce BMI-MVPA-Sedentary pattern ---\n",
    "if set([\"bmi_percentile\", \"v7_bmi_percentile\", \"mean_mvpa\", \"mean_sedentary\"]).issubset(synthetic_new.columns):\n",
    "    bmi_base = np.random.normal(50, 10, size=n_to_generate)\n",
    "    mvpa_noise = np.random.normal(0, 5, size=n_to_generate)\n",
    "    sed_noise = np.random.normal(0, 10, size=n_to_generate)\n",
    "\n",
    "    synthetic_new[\"mean_mvpa\"] = np.maximum(0, 90 - bmi_base + mvpa_noise)\n",
    "    synthetic_new[\"mean_sedentary\"] = np.maximum(100, 300 + bmi_base + sed_noise)\n",
    "    synthetic_new[\"bmi_percentile\"] = np.clip(bmi_base + np.random.normal(0, 3, size=n_to_generate), 15, 95)\n",
    "    synthetic_new[\"v7_bmi_percentile\"] = np.clip(synthetic_new[\"bmi_percentile\"] + np.random.normal(0, 2, size=n_to_generate), 15, 95)\n",
    "\n",
    "# --- Round fswitch and switch features to 0.25 increments ---\n",
    "for col in synthetic_new.columns:\n",
    "    if \"switch\" in col or \"fswitch\" in col:\n",
    "        synthetic_new[col] = (synthetic_new[col] / 0.25).round() * 0.25\n",
    "\n",
    "# --- Ensure bite/percent active eating slopes are non-negative ---\n",
    "for col in synthetic_new.columns:\n",
    "    if \"bite\" in col or \"percent_active_eating\" in col:\n",
    "        if \"intercept\" not in col:\n",
    "            synthetic_new[col] = synthetic_new[col].clip(lower=0)\n",
    "\n",
    "# --- Reinject missingness ---\n",
    "sampled_masks = resample(missing_mask.values, n_samples=n_to_generate, replace=True)\n",
    "synthetic_with_missing = synthetic_new.mask(sampled_masks)\n",
    "\n",
    "# --- Add noise and round to 2 decimals for numeric ---\n",
    "numeric_cols = df_data.select_dtypes(include=['number']).columns\n",
    "for col in numeric_cols:\n",
    "    if col in synthetic_with_missing.columns:\n",
    "        std = df_data[col].std()\n",
    "        if pd.notna(std) and std > 0:\n",
    "            synthetic_with_missing[col] += np.random.normal(0, 0.01 * std, size=n_to_generate)\n",
    "        synthetic_with_missing[col] = synthetic_with_missing[col].round(2)\n",
    "\n",
    "# --- Row validation ---\n",
    "def is_valid_row(row):\n",
    "    return 2 <= row.get(\"age_yr\", 0) <= 12 and row.get(\"v1_meal_total_kcal\", 0) <= 3500\n",
    "\n",
    "synthetic_valid = synthetic_with_missing[synthetic_with_missing.apply(is_valid_row, axis=1)].reset_index(drop=True)\n",
    "\n",
    "# --- Assign new IDs ---\n",
    "max_real_id = df_id.max()\n",
    "synthetic_valid.insert(0, \"id\", range(max_real_id + 1, max_real_id + 1 + len(synthetic_valid)))\n",
    "\n",
    "# --- Restore any previously dropped columns ---\n",
    "for col in all_nan_cols:\n",
    "    df_data[col] = np.nan\n",
    "    synthetic_valid[col] = np.nan\n",
    "\n",
    "# --- Combine original and synthetic data ---\n",
    "df_real_with_id = df_data.copy()\n",
    "df_real_with_id.insert(0, \"id\", df_id.values)\n",
    "\n",
    "df_combined = pd.concat([df_real_with_id, synthetic_valid], ignore_index=True)\n",
    "df_combined = df_combined.sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "# --- Save outputs ---\n",
    "df_combined.to_csv(\"ML_child_obesity_syn_data.csv\", index=False)\n",
    "print(\"✅ Final dataset saved as 'ML_child_obesity_syn_data.csv'\")\n",
    "print(\"✅ Metadata saved as 'ctgan_metadata.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
